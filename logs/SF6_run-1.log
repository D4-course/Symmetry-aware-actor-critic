2022-11-30 15:00:44.753 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cuda",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:01:12.884 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:01:12.885 INFO: Using CPU
2022-11-30 15:01:12.893 INFO: Training bags: ['SF6']
2022-11-30 15:01:12.893 INFO: Evaluation bags: ['SF6']
2022-11-30 15:01:13.271 INFO: [10] []
2022-11-30 15:01:13.303 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:01:13.305 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:01:13.307 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:01:13.465 INFO: Number of parameters: 176906
2022-11-30 15:01:13.477 INFO: Starting PPO
2022-11-30 15:01:13.478 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:01:20.139 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:05:31.270 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:05:31.270 INFO: Using CPU
2022-11-30 15:05:31.279 INFO: Training bags: ['SF6']
2022-11-30 15:05:31.279 INFO: Evaluation bags: ['SF6']
2022-11-30 15:05:31.444 INFO: [10] []
2022-11-30 15:05:31.446 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:05:31.447 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:05:31.449 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:05:31.458 INFO: Number of parameters: 176906
2022-11-30 15:05:31.468 INFO: Starting PPO
2022-11-30 15:05:31.469 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:05:38.213 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:06:00.559 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 15:06:02.366 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 15:06:02.477 INFO: Iteration: 1/106, steps: 140
2022-11-30 15:06:08.930 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 15:06:30.541 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 15:06:30.573 INFO: Iteration: 2/106, steps: 280
2022-11-30 15:06:37.134 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 15:25:30.521 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:25:30.522 INFO: Using CPU
2022-11-30 15:25:30.530 INFO: Training bags: ['SF6']
2022-11-30 15:25:30.530 INFO: Evaluation bags: ['SF6']
2022-11-30 15:25:30.703 INFO: [10] []
2022-11-30 15:25:30.704 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:25:30.706 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:25:30.707 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:25:30.718 INFO: Number of parameters: 176906
2022-11-30 15:25:30.729 INFO: Starting PPO
2022-11-30 15:25:30.729 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:25:38.319 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:26:04.190 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 15:26:06.074 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 15:26:06.181 INFO: Iteration: 1/106, steps: 140
2022-11-30 15:26:12.657 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 15:26:34.290 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 15:26:52.151 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:26:52.167 INFO: Using CPU
2022-11-30 15:26:52.176 INFO: Training bags: ['SF6']
2022-11-30 15:26:52.176 INFO: Evaluation bags: ['SF6']
2022-11-30 15:26:52.343 INFO: [10] []
2022-11-30 15:26:52.344 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:26:52.346 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:26:52.348 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:26:52.369 INFO: Number of parameters: 176906
2022-11-30 15:26:52.379 INFO: Starting PPO
2022-11-30 15:26:52.379 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:27:00.588 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:27:24.151 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 15:27:26.185 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 15:27:26.256 INFO: Iteration: 1/106, steps: 140
2022-11-30 15:27:32.945 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 15:27:56.548 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 15:27:56.565 INFO: Iteration: 2/106, steps: 280
2022-11-30 15:28:04.903 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 15:28:28.832 INFO: Optimization: policy loss=-0.000, vf loss=0.119, entropy loss=-0.007, total loss=0.112, num steps=1
2022-11-30 15:28:28.866 INFO: Iteration: 3/106, steps: 420
2022-11-30 15:28:36.284 INFO: Training rollout: return=-0.440 (0.2), episode length=3.5
2022-11-30 15:28:57.816 INFO: Optimization: policy loss=-0.000, vf loss=0.096, entropy loss=-0.007, total loss=0.089, num steps=1
2022-11-30 15:28:57.833 INFO: Iteration: 4/106, steps: 560
2022-11-30 15:29:04.638 INFO: Training rollout: return=-0.454 (0.1), episode length=3.4
2022-11-30 15:29:42.992 INFO: Optimization: policy loss=-0.011, vf loss=0.088, entropy loss=-0.007, total loss=0.070, num steps=2
2022-11-30 15:29:43.021 INFO: Iteration: 5/106, steps: 700
2022-11-30 15:29:51.300 INFO: Training rollout: return=-0.466 (0.1), episode length=3.2
2022-11-30 15:31:16.910 INFO: Optimization: policy loss=-0.048, vf loss=0.084, entropy loss=-0.007, total loss=0.029, num steps=7
2022-11-30 15:31:16.921 INFO: Iteration: 6/106, steps: 840
2022-11-30 15:31:24.567 INFO: Training rollout: return=-0.401 (0.1), episode length=3.5
2022-11-30 15:32:52.792 INFO: Optimization: policy loss=-0.057, vf loss=0.055, entropy loss=-0.007, total loss=-0.009, num steps=7
2022-11-30 15:32:52.807 INFO: Iteration: 7/106, steps: 980
2022-11-30 15:33:00.897 INFO: Training rollout: return=-0.452 (0.2), episode length=3.3
2022-11-30 15:36:30.681 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:36:30.682 INFO: Using CPU
2022-11-30 15:36:30.690 INFO: Training bags: ['SF6']
2022-11-30 15:36:30.690 INFO: Evaluation bags: ['SF6']
2022-11-30 15:36:30.860 INFO: [10] []
2022-11-30 15:36:30.862 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:36:30.865 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:36:30.867 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:36:30.876 INFO: Number of parameters: 176906
2022-11-30 15:36:30.887 INFO: Starting PPO
2022-11-30 15:36:30.887 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:36:38.856 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:37:00.900 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 15:37:02.607 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 15:37:02.672 INFO: Iteration: 1/106, steps: 140
2022-11-30 15:37:08.478 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 15:37:30.436 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 15:37:30.480 INFO: Iteration: 2/106, steps: 280
2022-11-30 15:37:37.351 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 15:38:00.527 INFO: Optimization: policy loss=-0.000, vf loss=0.119, entropy loss=-0.007, total loss=0.112, num steps=1
2022-11-30 15:38:00.552 INFO: Iteration: 3/106, steps: 420
2022-11-30 15:38:08.218 INFO: Training rollout: return=-0.440 (0.2), episode length=3.5
2022-11-30 15:38:30.786 INFO: Optimization: policy loss=-0.000, vf loss=0.096, entropy loss=-0.007, total loss=0.089, num steps=1
2022-11-30 15:38:30.793 INFO: Iteration: 4/106, steps: 560
2022-11-30 15:38:38.218 INFO: Training rollout: return=-0.454 (0.1), episode length=3.4
2022-11-30 15:39:17.833 INFO: Optimization: policy loss=-0.011, vf loss=0.088, entropy loss=-0.007, total loss=0.070, num steps=2
2022-11-30 15:39:17.858 INFO: Iteration: 5/106, steps: 700
2022-11-30 15:39:24.818 INFO: Training rollout: return=-0.466 (0.1), episode length=3.2
2022-11-30 15:45:09.408 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:45:09.409 INFO: Using CPU
2022-11-30 15:45:09.417 INFO: Training bags: ['SF6']
2022-11-30 15:45:09.417 INFO: Evaluation bags: ['SF6']
2022-11-30 15:45:09.569 INFO: [10] []
2022-11-30 15:45:09.571 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:45:09.572 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:45:09.573 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:45:09.582 INFO: Number of parameters: 176906
2022-11-30 15:45:09.592 INFO: Starting PPO
2022-11-30 15:45:09.592 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:45:16.590 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:45:38.459 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 15:45:40.154 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 15:45:40.222 INFO: Iteration: 1/106, steps: 140
2022-11-30 15:45:46.622 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 15:46:08.961 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 15:46:08.986 INFO: Iteration: 2/106, steps: 280
2022-11-30 15:46:15.182 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 15:46:37.655 INFO: Optimization: policy loss=-0.000, vf loss=0.119, entropy loss=-0.007, total loss=0.112, num steps=1
2022-11-30 15:46:37.683 INFO: Iteration: 3/106, steps: 420
2022-11-30 15:46:44.144 INFO: Training rollout: return=-0.440 (0.2), episode length=3.5
2022-11-30 15:47:06.109 INFO: Optimization: policy loss=-0.000, vf loss=0.096, entropy loss=-0.007, total loss=0.089, num steps=1
2022-11-30 15:47:06.128 INFO: Iteration: 4/106, steps: 560
2022-11-30 15:47:12.843 INFO: Training rollout: return=-0.454 (0.1), episode length=3.4
2022-11-30 15:47:46.759 INFO: Optimization: policy loss=-0.011, vf loss=0.088, entropy loss=-0.007, total loss=0.070, num steps=2
2022-11-30 15:47:46.772 INFO: Iteration: 5/106, steps: 700
2022-11-30 15:47:53.366 INFO: Training rollout: return=-0.466 (0.1), episode length=3.2
2022-11-30 15:49:10.545 INFO: Optimization: policy loss=-0.048, vf loss=0.084, entropy loss=-0.007, total loss=0.029, num steps=7
2022-11-30 15:49:10.578 INFO: Iteration: 6/106, steps: 840
2022-11-30 15:49:17.050 INFO: Training rollout: return=-0.401 (0.1), episode length=3.5
2022-11-30 15:50:32.930 INFO: Optimization: policy loss=-0.057, vf loss=0.055, entropy loss=-0.007, total loss=-0.009, num steps=7
2022-11-30 15:50:32.973 INFO: Iteration: 7/106, steps: 980
2022-11-30 15:50:40.799 INFO: Training rollout: return=-0.452 (0.2), episode length=3.3
2022-11-30 15:51:58.696 INFO: Optimization: policy loss=-0.049, vf loss=0.078, entropy loss=-0.007, total loss=0.023, num steps=7
2022-11-30 15:51:58.713 INFO: Iteration: 8/106, steps: 1120
2022-11-30 15:52:05.012 INFO: Training rollout: return=-0.441 (0.1), episode length=3.2
2022-11-30 15:53:28.539 INFO: Optimization: policy loss=-0.039, vf loss=0.045, entropy loss=-0.007, total loss=-0.001, num steps=7
2022-11-30 15:53:28.555 INFO: Iteration: 9/106, steps: 1260
2022-11-30 15:53:31.981 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 15:53:31.981 INFO: Using CPU
2022-11-30 15:53:31.998 INFO: Training bags: ['SF6']
2022-11-30 15:53:31.998 INFO: Evaluation bags: ['SF6']
2022-11-30 15:53:32.395 INFO: [10] []
2022-11-30 15:53:32.396 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:53:32.398 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 15:53:32.400 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 15:53:32.442 INFO: Number of parameters: 176906
2022-11-30 15:53:32.464 INFO: Starting PPO
2022-11-30 15:53:32.465 INFO: Iteration: 0/106, steps: 0
2022-11-30 15:53:44.557 INFO: Training rollout: return=-0.444 (0.1), episode length=3.1
2022-11-30 15:53:54.765 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 15:54:57.780 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 15:55:17.384 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 15:55:25.315 INFO: Iteration: 1/106, steps: 140
2022-11-30 15:55:42.090 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 15:57:16.422 INFO: Optimization: policy loss=-0.048, vf loss=0.047, entropy loss=-0.006, total loss=-0.007, num steps=7
2022-11-30 15:57:16.644 INFO: Iteration: 10/106, steps: 1400
2022-11-30 15:57:27.706 INFO: Training rollout: return=-0.442 (0.1), episode length=3.2
2022-11-30 15:58:45.821 INFO: Optimization: policy loss=-0.048, vf loss=0.051, entropy loss=-0.007, total loss=-0.004, num steps=7
2022-11-30 15:58:47.211 INFO: Evaluation rollout: return=-0.183 (0.0), episode length=5.0
2022-11-30 15:58:53.148 INFO: Iteration: 11/106, steps: 1540
2022-11-30 15:59:00.307 INFO: Training rollout: return=-0.472 (0.2), episode length=3.2
2022-11-30 16:00:22.308 INFO: Optimization: policy loss=-0.050, vf loss=0.042, entropy loss=-0.007, total loss=-0.015, num steps=7
2022-11-30 16:00:22.320 INFO: Iteration: 12/106, steps: 1680
2022-11-30 16:00:28.925 INFO: Training rollout: return=-0.437 (0.2), episode length=3.4
2022-11-30 16:02:02.689 INFO: Optimization: policy loss=-0.051, vf loss=0.031, entropy loss=-0.007, total loss=-0.027, num steps=7
2022-11-30 16:02:02.692 INFO: Iteration: 13/106, steps: 1820
2022-11-30 16:02:15.134 INFO: Training rollout: return=-0.416 (0.1), episode length=3.5
2022-11-30 16:04:06.337 INFO: Optimization: policy loss=-0.045, vf loss=0.022, entropy loss=-0.007, total loss=-0.029, num steps=7
2022-11-30 16:04:06.372 INFO: Iteration: 14/106, steps: 1960
2022-11-30 16:04:14.510 INFO: Training rollout: return=-0.458 (0.1), episode length=3.3
2022-11-30 16:05:41.639 INFO: Optimization: policy loss=-0.050, vf loss=0.028, entropy loss=-0.007, total loss=-0.030, num steps=7
2022-11-30 16:05:41.658 INFO: Iteration: 15/106, steps: 2100
2022-11-30 16:05:48.275 INFO: Training rollout: return=-0.403 (0.2), episode length=3.5
2022-11-30 16:07:06.524 INFO: Optimization: policy loss=-0.040, vf loss=0.022, entropy loss=-0.006, total loss=-0.024, num steps=7
2022-11-30 16:07:06.543 INFO: Iteration: 16/106, steps: 2240
2022-11-30 16:07:13.361 INFO: Training rollout: return=-0.407 (0.1), episode length=3.3
2022-11-30 16:08:34.834 INFO: Optimization: policy loss=-0.030, vf loss=0.014, entropy loss=-0.006, total loss=-0.022, num steps=7
2022-11-30 16:08:34.885 INFO: Iteration: 17/106, steps: 2380
2022-11-30 16:08:41.316 INFO: Training rollout: return=-0.407 (0.2), episode length=3.4
2022-11-30 16:10:05.597 INFO: Optimization: policy loss=-0.037, vf loss=0.024, entropy loss=-0.006, total loss=-0.019, num steps=7
2022-11-30 16:10:05.605 INFO: Iteration: 18/106, steps: 2520
2022-11-30 16:10:11.892 INFO: Training rollout: return=-0.420 (0.1), episode length=3.6
2022-11-30 16:11:38.464 INFO: Optimization: policy loss=-0.041, vf loss=0.024, entropy loss=-0.007, total loss=-0.024, num steps=7
2022-11-30 16:11:38.479 INFO: Iteration: 19/106, steps: 2660
2022-11-30 16:11:49.445 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 16:40:24.482 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 16:40:24.572 INFO: Using CPU
2022-11-30 16:40:24.606 INFO: Training bags: ['SF6']
2022-11-30 16:40:24.606 INFO: Evaluation bags: ['SF6']
2022-11-30 16:40:24.961 INFO: [10] []
2022-11-30 16:40:24.993 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 16:40:24.995 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 16:40:24.996 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 16:40:25.393 INFO: Number of parameters: 176906
2022-11-30 16:40:25.464 INFO: Starting PPO
2022-11-30 16:40:25.464 INFO: Iteration: 0/106, steps: 0
2022-11-30 16:40:32.522 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 16:41:02.025 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 16:41:04.121 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 16:41:04.442 INFO: Iteration: 1/106, steps: 140
2022-11-30 16:41:11.920 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 16:41:39.919 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 16:41:39.949 INFO: Iteration: 2/106, steps: 280
2022-11-30 16:41:48.939 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 16:42:16.213 INFO: Optimization: policy loss=-0.000, vf loss=0.119, entropy loss=-0.007, total loss=0.112, num steps=1
2022-11-30 16:42:16.237 INFO: Iteration: 3/106, steps: 420
2022-11-30 16:42:23.783 INFO: Training rollout: return=-0.440 (0.2), episode length=3.5
2022-11-30 16:42:50.531 INFO: Optimization: policy loss=-0.000, vf loss=0.096, entropy loss=-0.007, total loss=0.089, num steps=1
2022-11-30 16:42:50.580 INFO: Iteration: 4/106, steps: 560
2022-11-30 16:42:58.335 INFO: Training rollout: return=-0.454 (0.1), episode length=3.4
2022-11-30 16:43:49.625 INFO: Optimization: policy loss=-0.011, vf loss=0.088, entropy loss=-0.007, total loss=0.070, num steps=2
2022-11-30 16:43:49.645 INFO: Iteration: 5/106, steps: 700
2022-11-30 16:43:57.963 INFO: Training rollout: return=-0.466 (0.1), episode length=3.2
2022-11-30 16:46:19.073 INFO: Optimization: policy loss=-0.048, vf loss=0.084, entropy loss=-0.007, total loss=0.029, num steps=7
2022-11-30 16:46:19.164 INFO: Iteration: 6/106, steps: 840
2022-11-30 16:46:25.234 INFO: Training rollout: return=-0.401 (0.1), episode length=3.5
2022-11-30 16:47:41.453 INFO: Optimization: policy loss=-0.057, vf loss=0.055, entropy loss=-0.007, total loss=-0.009, num steps=7
2022-11-30 16:47:41.456 INFO: Iteration: 7/106, steps: 980
2022-11-30 16:47:49.058 INFO: Training rollout: return=-0.452 (0.2), episode length=3.3
2022-11-30 16:50:06.474 INFO: Optimization: policy loss=-0.049, vf loss=0.078, entropy loss=-0.007, total loss=0.023, num steps=7
2022-11-30 16:50:06.573 INFO: Iteration: 8/106, steps: 1120
2022-11-30 16:50:17.940 INFO: Training rollout: return=-0.441 (0.1), episode length=3.2
2022-11-30 16:51:46.443 INFO: Optimization: policy loss=-0.039, vf loss=0.045, entropy loss=-0.007, total loss=-0.001, num steps=7
2022-11-30 16:51:46.456 INFO: Iteration: 9/106, steps: 1260
2022-11-30 16:51:52.758 INFO: Training rollout: return=-0.444 (0.1), episode length=3.1
2022-11-30 16:53:14.292 INFO: Optimization: policy loss=-0.048, vf loss=0.047, entropy loss=-0.006, total loss=-0.007, num steps=7
2022-11-30 16:53:14.313 INFO: Iteration: 10/106, steps: 1400
2022-11-30 16:53:21.194 INFO: Training rollout: return=-0.442 (0.1), episode length=3.2
2022-11-30 16:54:35.350 INFO: Optimization: policy loss=-0.048, vf loss=0.051, entropy loss=-0.007, total loss=-0.004, num steps=7
2022-11-30 16:54:36.611 INFO: Evaluation rollout: return=-0.183 (0.0), episode length=5.0
2022-11-30 16:54:37.188 INFO: Iteration: 11/106, steps: 1540
2022-11-30 16:54:42.783 INFO: Training rollout: return=-0.472 (0.2), episode length=3.2
2022-11-30 16:55:56.454 INFO: Optimization: policy loss=-0.050, vf loss=0.042, entropy loss=-0.007, total loss=-0.015, num steps=7
2022-11-30 16:55:56.481 INFO: Iteration: 12/106, steps: 1680
2022-11-30 16:56:02.524 INFO: Training rollout: return=-0.437 (0.2), episode length=3.4
2022-11-30 16:57:17.771 INFO: Optimization: policy loss=-0.051, vf loss=0.031, entropy loss=-0.007, total loss=-0.027, num steps=7
2022-11-30 16:57:17.778 INFO: Iteration: 13/106, steps: 1820
2022-11-30 16:57:24.696 INFO: Training rollout: return=-0.416 (0.1), episode length=3.5
2022-11-30 16:58:45.014 INFO: Optimization: policy loss=-0.045, vf loss=0.022, entropy loss=-0.007, total loss=-0.029, num steps=7
2022-11-30 16:58:45.036 INFO: Iteration: 14/106, steps: 1960
2022-11-30 16:58:50.772 INFO: Training rollout: return=-0.458 (0.1), episode length=3.3
2022-11-30 17:00:02.306 INFO: Optimization: policy loss=-0.050, vf loss=0.028, entropy loss=-0.007, total loss=-0.030, num steps=7
2022-11-30 17:00:02.319 INFO: Iteration: 15/106, steps: 2100
2022-11-30 17:00:08.660 INFO: Training rollout: return=-0.403 (0.2), episode length=3.5
2022-11-30 17:01:26.270 INFO: Optimization: policy loss=-0.040, vf loss=0.022, entropy loss=-0.006, total loss=-0.024, num steps=7
2022-11-30 17:01:26.280 INFO: Iteration: 16/106, steps: 2240
2022-11-30 17:01:33.598 INFO: Training rollout: return=-0.407 (0.1), episode length=3.3
2022-11-30 17:02:57.678 INFO: Optimization: policy loss=-0.030, vf loss=0.014, entropy loss=-0.006, total loss=-0.022, num steps=7
2022-11-30 17:02:57.691 INFO: Iteration: 17/106, steps: 2380
2022-11-30 17:03:08.137 INFO: Training rollout: return=-0.407 (0.2), episode length=3.4
2022-11-30 17:04:51.086 INFO: Optimization: policy loss=-0.037, vf loss=0.024, entropy loss=-0.006, total loss=-0.019, num steps=7
2022-11-30 17:04:51.120 INFO: Iteration: 18/106, steps: 2520
2022-11-30 17:04:57.795 INFO: Training rollout: return=-0.420 (0.1), episode length=3.6
2022-11-30 17:06:16.903 INFO: Optimization: policy loss=-0.041, vf loss=0.024, entropy loss=-0.007, total loss=-0.024, num steps=7
2022-11-30 17:06:16.925 INFO: Iteration: 19/106, steps: 2660
2022-11-30 17:06:24.049 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 17:07:43.268 INFO: Optimization: policy loss=-0.037, vf loss=0.021, entropy loss=-0.007, total loss=-0.022, num steps=7
2022-11-30 17:07:43.281 INFO: Iteration: 20/106, steps: 2800
2022-11-30 17:07:49.830 INFO: Training rollout: return=-0.464 (0.1), episode length=3.4
2022-11-30 17:09:04.067 INFO: Optimization: policy loss=-0.037, vf loss=0.021, entropy loss=-0.007, total loss=-0.023, num steps=7
2022-11-30 17:09:05.668 INFO: Evaluation rollout: return=-0.384 (0.0), episode length=7.0
2022-11-30 17:09:05.744 INFO: Iteration: 21/106, steps: 2940
2022-11-30 17:09:11.903 INFO: Training rollout: return=-0.409 (0.2), episode length=3.7
2022-11-30 17:10:42.243 INFO: Optimization: policy loss=-0.047, vf loss=0.017, entropy loss=-0.007, total loss=-0.037, num steps=7
2022-11-30 17:10:42.263 INFO: Iteration: 22/106, steps: 3080
2022-11-30 17:10:48.896 INFO: Training rollout: return=-0.373 (0.1), episode length=3.6
2022-11-30 17:11:04.918 INFO: {
    "bag_scale": 5,
    "beta": "-10",
    "canvas_size": 7,
    "clip_ratio": 0.2,
    "data_dir": "data",
    "device": "cpu",
    "discount": 1.0,
    "entropy_coef": 0.01,
    "eval_formulas": null,
    "eval_freq": 10,
    "formulas": "SF6",
    "gradient_clip": 0.5,
    "keep_models": false,
    "lam": 0.97,
    "learning_rate": 0.0003,
    "load_latest": false,
    "load_model": null,
    "log_dir": "logs",
    "log_level": "INFO",
    "max_mean_distance": 2.1,
    "max_num_steps": 15000,
    "max_num_train_iters": 7,
    "max_solo_distance": 2.0,
    "maxl": 4,
    "min_atomic_distance": 0.6,
    "min_mean_distance": 1.1,
    "min_reward": -0.6,
    "mini_batch_size": 140,
    "model": "covariant",
    "model_dir": "models",
    "name": "SF6",
    "network_width": 128,
    "num_cg_levels": 3,
    "num_channels_hidden": 10,
    "num_channels_per_element": 4,
    "num_envs": 10,
    "num_eval_episodes": null,
    "num_gaussians": 3,
    "num_steps_per_iter": 140,
    "optimizer": "adam",
    "results_dir": "results",
    "save_freq": 10,
    "save_rollouts": "eval",
    "seed": 1,
    "symbols": "X,F,S",
    "target_kl": 0.01,
    "vf_coef": 0.5
}
2022-11-30 17:11:04.942 INFO: Using CPU
2022-11-30 17:11:04.951 INFO: Training bags: ['SF6']
2022-11-30 17:11:04.951 INFO: Evaluation bags: ['SF6']
2022-11-30 17:11:05.253 INFO: [10] []
2022-11-30 17:11:05.268 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 17:11:05.270 INFO: [10, 10, 10, 10, 10] [10, 10, 10, 10, 10]
2022-11-30 17:11:05.271 INFO: [12, 12, 12, 12, 12] [10, 10, 10, 10, 10]
2022-11-30 17:11:05.726 INFO: Number of parameters: 176906
2022-11-30 17:11:05.736 INFO: Starting PPO
2022-11-30 17:11:05.751 INFO: Iteration: 0/106, steps: 0
2022-11-30 17:11:36.802 INFO: Training rollout: return=-0.430 (0.1), episode length=3.4
2022-11-30 17:12:02.850 INFO: Optimization: policy loss=-0.006, vf loss=0.011, entropy loss=-0.006, total loss=-0.001, num steps=2
2022-11-30 17:12:02.983 INFO: Iteration: 23/106, steps: 3220
2022-11-30 17:12:26.121 INFO: Training rollout: return=-0.397 (0.2), episode length=3.7
2022-11-30 17:12:56.953 INFO: Optimization: policy loss=-0.000, vf loss=0.136, entropy loss=-0.007, total loss=0.129, num steps=1
2022-11-30 17:13:02.564 INFO: Evaluation rollout: return=-0.082 (0.0), episode length=7.0
2022-11-30 17:13:03.918 INFO: Iteration: 1/106, steps: 140
2022-11-30 17:13:31.878 INFO: Training rollout: return=-0.487 (0.2), episode length=3.2
2022-11-30 17:14:30.065 INFO: Optimization: policy loss=0.000, vf loss=0.143, entropy loss=-0.007, total loss=0.137, num steps=1
2022-11-30 17:14:30.110 INFO: Iteration: 2/106, steps: 280
2022-11-30 17:14:47.884 INFO: Training rollout: return=-0.447 (0.1), episode length=3.4
2022-11-30 17:15:11.210 INFO: Optimization: policy loss=-0.025, vf loss=0.016, entropy loss=-0.006, total loss=-0.015, num steps=5
2022-11-30 17:15:11.274 INFO: Iteration: 24/106, steps: 3360
2022-11-30 17:15:31.222 INFO: Training rollout: return=-0.402 (0.1), episode length=3.6
2022-11-30 17:15:32.995 INFO: Optimization: policy loss=-0.000, vf loss=0.119, entropy loss=-0.007, total loss=0.112, num steps=1
2022-11-30 17:15:33.032 INFO: Iteration: 3/106, steps: 420
2022-11-30 17:15:51.230 INFO: Training rollout: return=-0.440 (0.2), episode length=3.5
2022-11-30 17:16:43.545 INFO: Optimization: policy loss=-0.000, vf loss=0.096, entropy loss=-0.007, total loss=0.089, num steps=1
2022-11-30 17:16:43.555 INFO: Iteration: 4/106, steps: 560
2022-11-30 17:17:01.570 INFO: Training rollout: return=-0.454 (0.1), episode length=3.4
2022-11-30 17:17:34.079 INFO: Optimization: policy loss=-0.023, vf loss=0.011, entropy loss=-0.006, total loss=-0.017, num steps=4
2022-11-30 17:17:34.127 INFO: Iteration: 25/106, steps: 3500
2022-11-30 17:17:52.801 INFO: Training rollout: return=-0.326 (0.2), episode length=3.9
2022-11-30 17:18:22.610 INFO: Optimization: policy loss=-0.011, vf loss=0.088, entropy loss=-0.007, total loss=0.070, num steps=2
2022-11-30 17:18:22.687 INFO: Iteration: 5/106, steps: 700
2022-11-30 17:18:37.278 INFO: Training rollout: return=-0.466 (0.1), episode length=3.2
2022-11-30 17:21:19.252 INFO: Optimization: policy loss=-0.037, vf loss=0.010, entropy loss=-0.006, total loss=-0.033, num steps=7
2022-11-30 17:21:19.280 INFO: Iteration: 26/106, steps: 3640
2022-11-30 17:21:47.902 INFO: Training rollout: return=-0.370 (0.1), episode length=3.8
2022-11-30 17:22:07.018 INFO: Optimization: policy loss=-0.048, vf loss=0.084, entropy loss=-0.007, total loss=0.029, num steps=7
2022-11-30 17:22:07.040 INFO: Iteration: 6/106, steps: 840
2022-11-30 17:22:23.939 INFO: Training rollout: return=-0.401 (0.1), episode length=3.5
2022-11-30 17:22:59.939 INFO: Optimization: policy loss=-0.018, vf loss=0.015, entropy loss=-0.006, total loss=-0.009, num steps=2
2022-11-30 17:22:59.976 INFO: Iteration: 27/106, steps: 3780
2022-11-30 17:23:21.554 INFO: Training rollout: return=-0.333 (0.2), episode length=4.3
2022-11-30 17:25:23.109 INFO: Optimization: policy loss=-0.057, vf loss=0.055, entropy loss=-0.007, total loss=-0.009, num steps=7
2022-11-30 17:25:23.205 INFO: Iteration: 7/106, steps: 980
2022-11-30 17:25:50.146 INFO: Training rollout: return=-0.452 (0.2), episode length=3.3
2022-11-30 17:29:02.094 INFO: Optimization: policy loss=-0.056, vf loss=0.014, entropy loss=-0.006, total loss=-0.048, num steps=7
2022-11-30 17:29:02.223 INFO: Iteration: 28/106, steps: 3920
2022-11-30 17:29:22.295 INFO: Training rollout: return=-0.396 (0.2), episode length=3.9
2022-11-30 17:31:17.795 INFO: Optimization: policy loss=-0.013, vf loss=0.018, entropy loss=-0.006, total loss=-0.002, num steps=2
2022-11-30 17:31:17.940 INFO: Iteration: 29/106, steps: 4060
2022-11-30 17:31:42.562 INFO: Optimization: policy loss=-0.049, vf loss=0.078, entropy loss=-0.007, total loss=0.023, num steps=7
2022-11-30 17:31:42.672 INFO: Iteration: 8/106, steps: 1120
2022-11-30 17:31:44.486 INFO: Training rollout: return=-0.358 (0.2), episode length=3.8
2022-11-30 17:32:06.994 INFO: Training rollout: return=-0.441 (0.1), episode length=3.2
2022-11-30 17:34:54.432 INFO: Optimization: policy loss=-0.034, vf loss=0.014, entropy loss=-0.005, total loss=-0.025, num steps=5
2022-11-30 17:34:54.558 INFO: Iteration: 30/106, steps: 4200
2022-11-30 17:35:15.739 INFO: Training rollout: return=-0.346 (0.2), episode length=4.7
2022-11-30 17:35:48.206 INFO: Optimization: policy loss=-0.039, vf loss=0.045, entropy loss=-0.007, total loss=-0.001, num steps=7
2022-11-30 17:35:48.313 INFO: Iteration: 9/106, steps: 1260
2022-11-30 17:36:08.495 INFO: Training rollout: return=-0.444 (0.1), episode length=3.1
2022-11-30 17:37:04.479 INFO: Optimization: policy loss=-0.021, vf loss=0.024, entropy loss=-0.006, total loss=-0.003, num steps=3
2022-11-30 17:37:07.993 INFO: Evaluation rollout: return=-0.188 (0.0), episode length=5.0
2022-11-30 17:37:08.912 INFO: Iteration: 31/106, steps: 4340
2022-11-30 17:37:30.591 INFO: Training rollout: return=-0.386 (0.2), episode length=3.8
2022-11-30 17:39:03.974 INFO: Optimization: policy loss=-0.048, vf loss=0.047, entropy loss=-0.006, total loss=-0.007, num steps=7
2022-11-30 17:39:04.056 INFO: Iteration: 10/106, steps: 1400
2022-11-30 17:39:21.915 INFO: Training rollout: return=-0.442 (0.1), episode length=3.2
2022-11-30 17:40:25.315 INFO: Optimization: policy loss=-0.054, vf loss=0.014, entropy loss=-0.005, total loss=-0.045, num steps=7
2022-11-30 17:40:25.358 INFO: Iteration: 32/106, steps: 4480
2022-11-30 17:40:47.097 INFO: Training rollout: return=-0.313 (0.1), episode length=4.3
2022-11-30 17:42:19.837 INFO: Optimization: policy loss=-0.048, vf loss=0.051, entropy loss=-0.007, total loss=-0.004, num steps=7
2022-11-30 17:42:23.040 INFO: Evaluation rollout: return=-0.183 (0.0), episode length=5.0
2022-11-30 17:42:25.053 INFO: Iteration: 11/106, steps: 1540
2022-11-30 17:42:42.729 INFO: Training rollout: return=-0.472 (0.2), episode length=3.2
2022-11-30 17:42:57.016 INFO: Optimization: policy loss=-0.037, vf loss=0.014, entropy loss=-0.005, total loss=-0.029, num steps=4
2022-11-30 17:42:57.059 INFO: Iteration: 33/106, steps: 4620
2022-11-30 17:43:15.080 INFO: Training rollout: return=-0.341 (0.2), episode length=4.6
2022-11-30 17:45:33.130 INFO: Optimization: policy loss=-0.050, vf loss=0.042, entropy loss=-0.007, total loss=-0.015, num steps=7
2022-11-30 17:45:33.180 INFO: Iteration: 12/106, steps: 1680
2022-11-30 17:45:52.351 INFO: Training rollout: return=-0.437 (0.2), episode length=3.4
2022-11-30 17:46:06.574 INFO: Optimization: policy loss=-0.058, vf loss=0.015, entropy loss=-0.005, total loss=-0.048, num steps=7
2022-11-30 17:46:06.598 INFO: Iteration: 34/106, steps: 4760
2022-11-30 17:46:24.866 INFO: Training rollout: return=-0.332 (0.2), episode length=4.0
2022-11-30 17:48:44.411 INFO: Optimization: policy loss=-0.051, vf loss=0.031, entropy loss=-0.007, total loss=-0.027, num steps=7
2022-11-30 17:48:44.458 INFO: Iteration: 13/106, steps: 1820
2022-11-30 17:49:05.902 INFO: Training rollout: return=-0.416 (0.1), episode length=3.5
2022-11-30 17:49:23.000 INFO: Optimization: policy loss=-0.051, vf loss=0.012, entropy loss=-0.005, total loss=-0.045, num steps=6
2022-11-30 17:49:23.002 INFO: Iteration: 35/106, steps: 4900
2022-11-30 17:49:45.034 INFO: Training rollout: return=-0.315 (0.2), episode length=4.6
2022-11-30 17:51:54.543 INFO: Optimization: policy loss=-0.045, vf loss=0.022, entropy loss=-0.007, total loss=-0.029, num steps=7
2022-11-30 17:51:54.567 INFO: Iteration: 14/106, steps: 1960
2022-11-30 17:52:12.570 INFO: Training rollout: return=-0.458 (0.1), episode length=3.3
2022-11-30 17:52:26.289 INFO: Optimization: policy loss=-0.053, vf loss=0.011, entropy loss=-0.006, total loss=-0.048, num steps=7
2022-11-30 17:52:26.310 INFO: Iteration: 36/106, steps: 5040
2022-11-30 17:52:44.910 INFO: Training rollout: return=-0.273 (0.1), episode length=4.4
